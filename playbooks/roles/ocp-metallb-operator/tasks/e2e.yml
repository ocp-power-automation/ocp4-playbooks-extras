---

# e2e testcases metallb-operator
- debug:
    msg: Print the value in vars {{ metallb_test_image }}

- name: Validate the controller and speaker pods can be scheduled on the tainted nodes based on the tolerations in metallb CR
  block:
  - name: Check the number of nodes running
    shell: oc get node | grep "Ready" | wc -l
    register: metallb_nodes

  - name: Get the master node name
    shell: oc get nodes | grep master | head -1 | awk '{if ($1 ~ /master/) print $1}'
    register: master_node
    failed_when: master_node.stdout_lines|length < 1

  - name: Get the worker node name
    shell: oc get nodes | grep worker | head -1 | awk '{if ($1 ~ /worker/) print $1}'
    register: worker_node
    failed_when: worker_node.stdout_lines|length < 1

  - name: Taint master and worker nodes
    shell: |
      oc adm taint node {{ master_node.stdout }} master-group=infra:NoSchedule
      oc adm taint node {{ worker_node.stdout }} worker-group=infra:NoSchedule
    register: taint_output
    ignore_errors: true

  - name: Create a metalLB CR with tolerations for worker-group in both controller and speaker pods
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerTolerations:
            - effect: NoSchedule
              key: worker-group
              operator: Equal
              value: infra
          logLevel: debug
          speakerTolerations:
            - effect: NoSchedule
              key: worker-group
              operator: Equal
              value: infra

  - name: Check if all the pods are running

    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 30

  - name: Check if the deployment for the controller is running

    shell: oc get deployment -n "{{ metallb_namespace }}" controller | awk '{if ($1 ~ /controller/) print $4}'
    register: metallb_controller
    until: metallb_controller.stdout|int == 1
    retries: 10
    delay: 40

  - name: Check if the daemon set are created properly
    shell: oc get daemonset -n "{{ metallb_namespace }}" speaker | awk '{if ($1 ~ /speaker/) print $6}'
    register: metallb_speaker
    until: metallb_speaker.stdout|int == metallb_nodes.stdout|int-1
    retries: 10
    delay: 40

  - name: Get the instance of the metallb operator
    shell: oc get metallb -n "{{ metallb_namespace }}" -ojsonpath="{.items[].metadata.name}"
    register: metallb_instance
    ignore_errors: true

  - name: Delete the MetalLB CR which has toleration for worker group
    kubernetes.core.k8s:
      state: absent
      api_version: metallb.io/v1beta1
      kind: MetalLB
      name: "{{ metallb_instance.stdout }}"
      namespace: "{{ metallb_namespace }}"
    when: metallb_instance.stdout|length > 0

  - name: Check if the speakers are deleted
    shell: oc get daemonset -n "{{ metallb_namespace }}" speaker
    register: metallb_speaker
    ignore_errors: true
    failed_when: "'NotFound' not in metallb_speaker.stderr"

  - name: Create a metalLB CR with tolerations for master-group in both controller and speaker pods
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerTolerations:
            - effect: NoSchedule
              key: master-group
              operator: Equal
              value: infra
          logLevel: debug
          speakerTolerations:
            - effect: NoSchedule
              key: master-group
              operator: Equal
              value: infra

  - name: Check if the daemon set are created properly
    shell: oc get daemonset -n "{{ metallb_namespace }}" speaker | awk '{if ($1 ~ /speaker/) print $6}'
    register: metallb_speaker
    until: metallb_speaker.stdout|int == metallb_nodes.stdout|int-1
    retries: 10
    delay: 40

  - name: Verify that no speaker pod is running on tainted worker node
    shell: oc get pod -n "{{ metallb_namespace }}" -o wide | awk '{if (($1 ~ /speaker/) && ($7=="{{ worker_node.stdout }}")) print $7}' | wc -l
    register: metallb_speaker
    failed_when: metallb_speaker.stdout|int > 0

  - name: Untaint master and worker nodes
    shell: |
      oc adm taint node {{ master_node.stdout }} master-group=infra:NoSchedule-
      oc adm taint node {{ worker_node.stdout }} worker-group=infra:NoSchedule-
    register: taint_output
    ignore_errors: true

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml
  when: metallb_e2e

- name: Validate controller and pod can be scheduled based on node selectors
  block:
  - name: Check if the pods are running properly
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep "Running\|Completed" | wc -l
    register: metallb_pods
    failed_when: metallb_pods.stdout|int != 2

  - name: Get the worker node name
    shell: oc get nodes | grep worker | head -1 | awk '{if ($1 ~ /worker/) print $1}'
    register: worker_node
    failed_when: worker_node.stdout_lines|length < 1

  - name: Create a node selector CR
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerNodeSelector:
            kubernetes.io/hostname: "{{ worker_node.stdout }}"
          nodeSelector:
            node-role.kubernetes.io/worker: "{{ worker_node.stdout }}"

  - name: Check if the controller pod is getting created only on the mentioned worker based on node selector
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /^controller/) print $7}'
    register: metallb_controller_pod
    failed_when: worker_node.stdout not in metallb_controller_pod.stdout

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml
  when: metallb_e2e

- name: Validate controller and speaker pods can be scheduled based on affinity
  block:

  - name: Get the worker node name
    shell: oc get nodes | grep worker | head -2 | awk '{if ($1 ~ /worker/) print $1}'
    register: worker_nodes
    failed_when: worker_nodes.stdout_lines|length != 2

  - name: Label two worker nodes with labels zone=east and zone west respectively
    shell: |
      oc label node {{ worker_nodes.stdout_lines[0] }} zone=east
      oc label node {{ worker_nodes.stdout_lines[1] }} zone=west

  - name: Create a metallb CR with controller and speaker pod based on (required) node affinity matchExpression
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerConfig:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: zone
                          operator: In
                          values:
                            - east
            speakerConfig:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: zone
                            operator: In
                            values:
                              - west

  - name: Check if the controller is getting created on worker 0
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /^controller/) print $7}'
    register: metallb_controller_pod
    until: worker_nodes.stdout_lines[0] in metallb_controller_pod.stdout
    retries: 5
    delay: 20

  - name: Check if the speaker pod is getting created on worker 1
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /^speaker/) print $7}'
    register: metallb_speaker_pod
    until: worker_nodes.stdout_lines[1] in metallb_speaker_pod.stdout
    retries: 5
    delay: 20

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml

  # Create a metalLB CR based on node affinity using matchFields
  - name: Create a metallb CR with controller and speaker pod based on ( required) node affinity using matchFields
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerConfig:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchFields:
                        - key: metadata.name
                          operator: In
                          values:
                            - "{{ worker_nodes.stdout_lines[0] }}"
            speakerConfig:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchFields:
                          - key: metadata.name
                            operator: In
                            values:
                              - "{{ worker_nodes.stdout_lines[1] }}"

  # Verification of node affinity using matchFields
  - name: Check if the controller is getting created on worker 0
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /^controller/) print $7}'
    register: metallb_controller_pod
    until: worker_nodes.stdout_lines[0] in metallb_controller_pod.stdout
    retries: 5
    delay: 20

  - name: Check if the speaker pod is getting created on worker 1
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /^speaker/) print $7}'
    register: metallb_speaker_pod
    until: worker_nodes.stdout_lines[1] in metallb_speaker_pod.stdout
    retries: 5
    delay: 20

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml

  # Create a metalLB CR to ensure pod affinity is honored for controller and speaker
  - name: Create a sample nginx pod in same namespace
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Pod
        metadata:
          name: nginx-test
          namespace: "{{ metallb_namespace }}"
          labels:
            name: test-only-pod
        spec:
          containers:
            - name: nginx-test
              image: nginx

  - name: Create a metallb CR with controller and speaker pod based on ( required) node affinity using matchFields
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerConfig:
            affinity:
              podAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: name
                          operator: In
                          values:
                            - test-only-pod
                    topologyKey: kubernetes.io/hostname

          logLevel: debug
          speakerConfig:
            affinity:
              podAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: name
                          operator: In
                          values:
                            - test-only-pod
                    topologyKey: kubernetes.io/hostname

  # Verification of pod affinity
  - name: Get the node on where nginx is run
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if ($1 ~ /nginx/) print $7}'
    register: nginx_node

  - name: Check if the controller is getting created on same node as per the label mentioned in CR
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if (($1 ~ /^controller/) && ($3=="Running")) print $7}'
    register: metallb_controller_node
    until: metallb_controller_node.stdout == nginx_node.stdout
    retries: 5
    delay: 20

  - name: Check if the speaker pod is getting created on same node as per the label mentioned in CR
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if (($1 ~ /^speaker/) && ($3=="Running")) print $7}'
    register: metallb_speaker_node
    until: metallb_speaker_node.stdout == nginx_node.stdout
    retries: 5
    delay: 20

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml

  # Create metalLB CR with pod anti affinity rule
  - name: Create a metallb CR with with pod anti affinity rule
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerConfig:
            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: name
                          operator: In
                          values:
                            - test-only-pod
                    topologyKey: kubernetes.io/hostname
          speakerConfig:
            nodeSelector:
              node-role.kubernetes.io/worker: ''
            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: name
                          operator: In
                          values:
                            - test-only-pod
                    topologyKey: kubernetes.io/hostname

  # Verification
  - name: Check if the controller is getting created on a different node than the pod with label
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if (($1 ~ /^controller/) && ($3=="Running")) print $7}'
    register: metallb_controller_node
    until: metallb_controller_node.stdout != nginx_node.stdout
    retries: 5
    delay: 20

  - name: Check if the speakers are getting created on a different node than the pod with label
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide --no-headers | awk '{if (($1 ~ /^speaker/) && ($3=="Running")) print $7}'
    register: metallb_speaker_node
    failed_when: nginx_node.stdout in metallb_speaker_node.stdout

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml

  - name: Delete the nginx pod created during the test
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Pod
      name: nginx-test
      namespace: "{{ metallb_namespace }}"
  when: metallb_e2e

# Validate Community creation and update
- name: Validate Community creation and update
  block:
  - name: Create a community name community1
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: Community
        metadata:
          name: community1
          namespace: "{{ metallb_namespace }}"
        spec:
          communities:
            - name: NO_ADVERTISE
              value: 65535:65282

  # Edit the community1 to add same entry of community to see error
  - name: Add same entry of community
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: Community
        metadata:
          name: community1
          namespace: "{{ metallb_namespace }}"
        spec:
          communities:
            - name: NO_ADVERTISE
              value: 65535:65282
            - name: NO_ADVERTISE
              value: 65535:65282
    register: community_output
    failed_when: "'duplicate definition of community' not in community_output.msg"
    ignore_errors: true
  when: metallb_e2e

# Verify L2 LoadBalancer service type are created on non cloud clusters
- name: Verify L2 LoadBalancer service type are created on non cloud clusters
  block:
  - name: Create CR for metallb on worker node
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          nodeSelector:
            node-role.kubernetes.io/worker: ''

  - name: Check if all the pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 30

  - name: Delete the project if it exists already
    shell: oc delete project test1
    ignore_errors: true

  - name: Create a project
    shell: oc new-project test1

  - name: Configure an address pool in the namespace where metallb is installed
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-silver
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[0] }}-{{ l2_address[0] }}"
            - "{{ l2_address[1] }}-{{ l2_address[1] }}"

  - name: Create a replica set and service of type LoadBalanncer
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: hello-world
          namespace: test1
          labels:
            app: hello-world
        spec:
          selector:
            matchLabels:
              app: hello-world
          replicas: 2
          template:
            metadata:
              labels:
                app: hello-world
            spec:
              containers:
                - name: hello-world
                  image: "{{ metallb_test_image }}"
                  imagePullPolicy: Always
                  ports:
                    - containerPort: 8080
                      protocol: TCP

  - name: Create a replica set and service of type LoadBalanncer
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Service
        metadata:
          name: hello-world
          namespace: test1
          annotations:
            metallb.universe.tf/address-pool: address-pool-silver
        spec:
          selector:
            app: hello-world
          ports:
            - port: 80
              targetPort: 8080
              protocol: TCP
          type: LoadBalancer

  - name: Check if all the pods are running
    shell: oc get pods -n test1 --no-headers | grep "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 2
    retries: 5
    delay: 20

  - name: Check if the replicaset is proper
    shell: oc get rs -n test1 --no-headers | awk '{if ($1 ~ /^hello/) print $4}'
    register: metallb_rs
    failed_when: metallb_rs.stdout|int != 2

  - name: Check if the loadbalancer service is proper
    shell: oc get svc -n test1 --no-headers | awk '{if ($2 ~ /LoadBalancer/)  print $4}'
    register: metallb_svc
    until: metallb_svc.stdout != "<pending>"
    retries: 10
    delay: 40

  # Verification
  - name: Check if the service is reachable
    shell: oc debug node/master-0 -- chroot /host curl -I http://{{ metallb_svc.stdout }}
    register: metallb_svc
    failed_when: "'200 OK' not in metallb_svc.stdout"

  #Delete the service and replication controllers
  - name: Delete the IP address pool
    kubernetes.core.k8s:
      state: absent
      api_version: metallb.io/v1beta1
      kind: IPAddressPool
      name: address-pool-silver
      namespace: "{{ metallb_namespace }}"

  - name: Delete the replication controllers
    kubernetes.core.k8s:
      state: absent
      api_version: apps/v1
      kind: ReplicaSet
      name: hello-world
      namespace: test1

  - name: Delete the Service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: hello-world
      namespace: test1

  - name: Delete the project
    shell: oc delete project test1

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml
  when: metallb_e2e

# Validate controller and speaker pods are scheduled on nodes based priority class and run time class
- name: Validate controller and speaker pods are scheduled on nodes based priority class and run time class
  block:
  - name: Check if all the pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 60

  # Create a metalLB CR with priority class set
  - name: Create a metalLB CR with priority class set
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          controllerConfig:
            priorityClassName: high-priority
          speakerConfig:
            priorityClassName: high-priority

  # Verify that the speaker and controller pods are not created as the priority class doesnt exist yet
  - name: Check if all the pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep "speaker\|^controller" | wc -l
    register: metallb_pods
    failed_when: metallb_pods.stdout|int != 0

  # Create the priority class to see controller and speaker pods are created
  - name: Create the priority class to see controller and speaker pods are created
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: scheduling.k8s.io/v1
        kind: PriorityClass
        metadata:
          name: high-priority
        value: 1000000
        globalDefault: false
        description: "This priority class should be used for metallb controller and speaker pods only."

  - name: Get the priority class
    shell: oc get priorityclass --no-headers | grep "high-priority" | wc -l
    register: priorityclass_op
    failed_when: priorityclass_op.stdout|int != 1

  - name: Check if the speaker and controller pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep "speaker\|^controller" | wc -l
    register: metallb_pods
    failed_when: metallb_pods.stdout|int < 0

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml

  - name: Delete the priority class
    kubernetes.core.k8s:
      state: absent
      api_version: scheduling.k8s.io/v1
      kind: PriorityClass
      name: high-priority
  when: metallb_e2e

# Verify static address is associated with LoadBalancer service if it is specified in YAML
- name: Verify static address is associated with LoadBalancer service if it is specified in YAML
  block:
  - name: Create a metalLB CR
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"

  - name: Create a address pool with set of IP addresses intended to be used as static IP using autoAssign False
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-static
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[0] }}-{{ l2_address[0] }}"
          autoAssign: false

  - name: Check if the addresspool is proper
    shell: oc get IPaddresspool -n "{{ metallb_namespace }}" | grep "address-pool-static" | wc -l
    register: metallb_adpool
    failed_when: metallb_adpool.stdout|int != 1

  - name: Delete the project if it exists already
    shell: oc delete project test
    ignore_errors: true

  - name: Create a new project test
    shell: oc new-project test

  - name: Create a service of type LoadBalancer using one of the IP addresses of node for EXTERNAL_IP
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: List
        items:
          - apiVersion: v1
            kind: ReplicationController
            metadata:
              labels:
                name: hello-idle
              name: hello-idle
              namespace: test
            spec:
              replicas: 2
              selector:
                name: hello-idle
              template:
                metadata:
                  labels:
                    name: hello-idle
                spec:
                  containers:
                    - image: "{{ metallb_test_image }}"
                      name: hello-idle
                      ports:
                        - containerPort: 8080
                          protocol: TCP
                      resources:
                        limits:
                          cpu: 200m
                          memory: 256Mi
                        requests:
                          cpu: 100m
                          memory: 256Mi
                      terminationMessagePath: /dev/termination-log
                  dnsPolicy: ClusterFirst
                  restartPolicy: Always
                  securityContext: { }
                  terminationGracePeriodSeconds: 30
          - apiVersion: v1
            kind: Service
            metadata:
              name: hello-idle
              namespace: test
              annotations:
                metallb.universe.tf/address-pool: address-pool-static
            spec:
              ports:
                - port: 80
                  targetPort: 8080
                  protocol: TCP
              selector:
                name: hello-idle
              type: LoadBalancer
              loadBalancerIP: "{{ l2_address[0] }}"

  - name: Check the service EXTERNAL_IP
    shell: oc get svc --no-headers | awk '{if ($1 ~ /^hello/) print $4}'
    register: svc_ip
    until: svc_ip.stdout == "{{ l2_address[0] }}"
    retries: 10
    delay: 40

  #Delete the service and replication controllers
  - name: Delete the IP address pool
    kubernetes.core.k8s:
      state: absent
      api_version: metallb.io/v1beta1
      kind: IPAddressPool
      name: address-pool-static
      namespace: "{{ metallb_namespace }}"

  - name: Delete the replication controllers
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: ReplicationController
      name: hello-idle
      namespace: test

  - name: Delete the Service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: hello-idle
      namespace: test

  - name: Delete the project
    shell: oc delete project test

  when: metallb_e2e

# Verify the logging level of MetalLB can be changed for debugging
- name: Verify the logging level of MetalLB can be changed for debugging
  block:
  - name: Create a metalLB CR
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"

  - name: Check if all the pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 60

  - name: Check the clusterserviceversion
    shell: oc get csv -n "{{ metallb_namespace }}" --no-headers | grep "metallb" | wc -l
    register: metallb_csv
    failed_when: metallb_csv.stdout|int != 1

  - name: Fetch one speaker pod name
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | awk '{if ($1 ~ /^speaker/) {print$1;exit;}}'
    register: speaker_pod

  - name: Check the current log level
    shell: oc get pods {{ speaker_pod.stdout }} -n "{{ metallb_namespace }}" -oyaml | grep log-level
    register: log_level
    failed_when: "'info' not in log_level.stdout"

  - name: Update log level to debug
    shell: oc patch --type=merge metallbs.metallb.io metallb -p '{"spec":{"logLevel":"debug"}}' -n "{{ metallb_namespace }}"

  - name: Sleep for 120 seconds for the speaker pods to be recreated
    wait_for:
      delay: 120
      timeout: 0

  - name: Wait for the speaker pods to be recreated
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | awk '{if ($1 ~ /^speaker/) {print$1;exit;}}'
    register: speaker_pod

  - name: Check if the log level is changed to debug
    shell: oc get pods {{ speaker_pod.stdout }} -n "{{ metallb_namespace }}" -oyaml | grep log-level
    register: log_level
    failed_when: "'debug' not in log_level.stdout"

  - name: Check the logs in 'speaker' container of speaker pod to see log level debug
    shell: oc logs {{ speaker_pod.stdout }} -n "{{ metallb_namespace }}" | grep "debug" | tail -10 | wc -l
    register: log_output
    failed_when: log_output.stdout|int < 0
  when: metallb_e2e

# The L2 service with externalTrafficPolicy Local continues to service requests even when node with endpoints goes down
# Can't add this testcase in CI as this involves rebooting of worker node while continuously sending requests to master node

# Verify one address can be associated with more than one service using annotation metallb.universe.tf/allow-shared-ip
- name: Verify one address can be associated with more than one service using annotation metallb.universe.tf/allow-shared-ip
  block:
  - name: Create IP Address pool
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-static
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[0] }}-{{ l2_address[0] }}"

  - name: Delete the project if it exists already
    shell: oc delete project test
    ignore_errors: true

  - name: Create new project
    shell: oc new-project test

  - name: Create two services with same set of backend pods and same IP address but different protocol in project test
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: List
        items:
          - apiVersion: v1
            kind: ReplicationController
            metadata:
              labels:
                name: web-server-rc
              name: web-server-rc
              namespace: test
            spec:
              replicas: 2
              template:
                metadata:
                  labels:
                    name: web-server-rc
                spec:
                  containers:
                    - image: "{{ metallb_test_image }}"
                      name: nginx
          - apiVersion: v1
            kind: Service
            metadata:
              labels:
                name: service-secure
              name: service-secure
              namespace: test
              annotations:
                metallb.universe.tf/IPAddress-pool: address-pool-static
                metallb.universe.tf/allow-shared-ip: "web-server-svc"
            spec:
              ports:
                - name: https
                  port: 27443
                  protocol: TCP
                  targetPort: 8443
              selector:
                name: web-server-rc
              type: LoadBalancer
              loadBalancerIP: "{{ l2_address[0] }}"
          - apiVersion: v1
            kind: Service
            metadata:
              labels:
                name: service-unsecure
              name: service-unsecure
              namespace: test
              annotations:
                metallb.universe.tf/IPAddress-pool: address-pool-static
                metallb.universe.tf/allow-shared-ip: "web-server-svc"
            spec:
              ports:
                - name: http
                  port: 27017
                  protocol: TCP
                  targetPort: 8080
              selector:
                name: web-server-rc
              type: LoadBalancer
              loadBalancerIP: "{{ l2_address[0] }}"

  - name: Check if the services are created
    shell: oc get svc -n test | grep {{ l2_address[0] }} | wc -l
    register: svc_count
    failed_when: svc_count.stdout|int != 2

  # verify reachability using curl command from master node
  - name: Check if the service is reachable
    shell: oc debug node/master-0 -- chroot /host curl -I http://{{ l2_address[0] }}:27017
    register: metallb_svc
    failed_when: "'200 OK' not in metallb_svc.stdout"

  - name: Delete the IP address pool
    kubernetes.core.k8s:
      state: absent
      api_version: metallb.io/v1beta1
      kind: IPAddressPool
      name: address-pool-static
      namespace: "{{ metallb_namespace }}"

  - name: Delete the replication controller
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: ReplicationController
      name: web-server-rc
      namespace: test

  - name: Delete the secure service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: service-secure
      namespace: test

  - name: Delete the unsecure service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: service-unsecure
      namespace: test

  - name: Delete the project test
    shell: oc delete project test
  when: metallb_e2e

# Validate L2 advertisement creation and update
- name: Validate L2 advertisement creation and update
  block:
  - name: Check if all the pods are running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 60

  - name: Get the worker node name
    shell: oc get nodes | grep worker | head -1 | awk '{if ($1 ~ /worker/) print $1}'
    register: worker_node
    failed_when: worker_node.stdout_lines|length < 1

  - name: Create IP Address pool
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: ip-addresspool-l2
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[0] }}/32"
          autoAssign: true

  - name: Create another IP Address pool
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          labels:
            zone: east
          name: ip-addresspool-l2-a
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[1] }}/32"
          autoAssign: true

  - name: Verify the IP Address pool created
    shell: oc get ipaddresspool -n "{{ metallb_namespace }}" --no-headers | grep "ip-addresspool" | wc -l
    register: address_pool
    failed_when: address_pool.stdout|int != 2

  - name: Create L2 advertisement
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"
        spec:
          ipAddressPoolSelectors:
            - matchExpressions:
                - key: zone
                  operator: In
                  values:
                    - east

  - name: Validate if the L2Advertisement is successfully created
    shell: oc get l2advertisement -n "{{ metallb_namespace }}" --no-headers | grep l2-adv | wc -l
    register: l2adv
    failed_when: l2adv.stdout|int != 1

  - name: Edit L2 advertisement object with address pool
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"
        spec:
          ipAddressPoolSelectors:
            - matchExpressions:
                - key: zone
                  operator: In
                  values:
                    - east
          ipAddressPools:
            - ip-addresspool-l2

  - name: Edit L2 advertisement object with same ip address pool name to see error
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"
        spec:
          ipAddressPoolSelectors:
            - matchExpressions:
                - key: zone
                  operator: In
                  values:
                    - east
          ipAddressPools:
            - ip-addresspool-l2
            - ip-addresspool-l2
    register: l2adverror
    ignore_errors: true
    failed_when: "'duplicate definition' not in l2adverror.msg"

  - name: Edit L2 advertisement object to add node selector
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"
        spec:
          ipAddressPoolSelectors:
            - matchExpressions:
                - key: zone
                  operator: In
                  values:
                    - east
          ipAddressPools:
            - ip-addresspool-l2
          nodeSelectors:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - "{{ worker_node.stdout }}"

  - name: Edit L2 advertisement object to add same node to see error
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"
        spec:
          ipAddressPoolSelectors:
            - matchExpressions:
                - key: zone
                  operator: In
                  values:
                    - east
          ipAddressPools:
            - ip-addresspool-l2
          nodeSelectors:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - "{{ worker_node.stdout }}"
                    - "{{ worker_node.stdout }}"
    register: l2adverror
    ignore_errors: true
    failed_when: "'duplicate definition' not in l2adverror.msg"

  # Delete the L2 advertisement and addresspool
  - name: Delete L2 advertisement
    kubernetes.core.k8s:
      state: absent
      definition:
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2-adv
          namespace: "{{ metallb_namespace }}"

  - name: Delete L2 address pool
    kubernetes.core.k8s:
      state: absent
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: ip-addresspool-l2
          namespace: "{{ metallb_namespace }}"

  - name: Delete L2 address pool
    kubernetes.core.k8s:
      state: absent
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: ip-addresspool-l2-a
          namespace: "{{ metallb_namespace }}"

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml
  when: metallb_e2e

# Verify the LoadBalancer service uses the right protocol based on address available from L2 or BGP address pool
- name: Verify the LoadBalancer service uses the right protocol based on address available from L2 or BGP address pool
  block:
  - name: Install metallb speakers only on the worker nodes by creating the metallb CR.
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: MetalLB
        metadata:
          name: metallb
          namespace: "{{ metallb_namespace }}"
        spec:
          nodeSelector:
            node-role.kubernetes.io/worker: ""
  - name: Verify the speakers and controller pods running
    shell: oc get pods -n "{{ metallb_namespace }}" --no-headers | grep -v "Running\|Completed" | wc -l
    register: metallb_pods
    until: metallb_pods.stdout|int == 0 and metallb_pods.stderr == ""
    retries: 10
    delay: 60

  - name: Configure FRR on bastion
    shell: |
      FRRVER="{{ metallb_frr_ver }}"
      curl -O "{{ metallb_frr_rpm }}"
      sudo yum install ./$FRRVER*
      sudo yum install frr -y
      systemctl status frr
      systemctl restart frr
      systemctl status frr
    register: frrconf

  - name: Open port 179 on bastion
    shell: |
      sudo firewall-cmd --add-port=179/tcp
      sudo firewall-cmd --add-port=179/udp

  # Add conf and zebra files
  # Get the speaker pods ip address
  - name: Verify the speakers and controller pods running
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide | awk '{if ($1 ~ /^speaker/) print $6}'
    register: speaker_pods
    failed_when: speaker_pods.stdout_lines|length != 2

  - name: Add conf details
    blockinfile:
      path: /etc/frr/frr.conf
      block: |
        debug bgp updates
        debug bgp neighbor
        debug bgp nht
        debug bfd peer
        log file /tmp/frr.log debugging
        log timestamp precision 3
        
        router bgp 64521
         bgp router-id {{ ansible_default_ipv4.address }}
         timers bgp 3 15
         no bgp ebgp-requires-policy
         no bgp default ipv4-unicast
         no bgp network import-check
         neighbor ocpmetallb peer-group
         neighbor ocpmetallb remote-as 64521
          neighbor {{ speaker_pods.stdout_lines[0] }} peer-group ocpmetallb
          neighbor {{ speaker_pods.stdout_lines[1] }} peer-group ocpmetallb
        
         address-family ipv4 unicast
          neighbor {{ speaker_pods.stdout_lines[0] }} next-hop-self
          neighbor {{ speaker_pods.stdout_lines[0] }} activate
          neighbor {{ speaker_pods.stdout_lines[1] }} next-hop-self
          neighbor {{ speaker_pods.stdout_lines[1] }} activate
      state: present
      backup: yes

  - name: Add details in zebra.conf
    copy:
      dest: /etc/frr/zebra.conf
      content: |
        hostname {{ ansible_hostname }}
        debug zebra nht
        interface eth0
         ip address {{ ansible_default_ipv4.address }}/22

  - name: Create IP Address pool for layer 2
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-silver-l2
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: layer2
          addresses:
            - "{{ l2_address[0] }}-{{ l2_address[0] }}"

  - name: Create IP Address pool for bgp
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-silver-bgp
          namespace: "{{ metallb_namespace }}"
        spec:
          protocol: bgp
          autoAssign: true
          addresses:
            - "{{ bgp_address[0] }}/32"

  - name: Create BGP Peer
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: metallb.io/v1beta2
        kind: BGPPeer
        metadata:
          name: peer-no-bfd
          namespace: "{{ metallb_namespace }}"
        spec:
          myASN: 65500
          peerASN: 64521
          peerAddress: "{{ ansible_default_ipv4.address }}"

  - name: Check if the bgppeer is created successfully
    shell: oc get bgppeer -n "{{ metallb_namespace }}" --no-headers | wc -l
    register: bgppeer
    failed_when: bgppeer.stdout|int != 1

  - name: Delete the project if it exists already
    shell: oc delete project test
    ignore_errors: true

  - name: Create a project
    shell: oc new-project test

  - name: Create a replica set
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: hello-world
          namespace: test
          labels:
            app: hello-world
        spec:
          selector:
            matchLabels:
              app: hello-world
          replicas: 2
          template:
            metadata:
              labels:
                app: hello-world
            spec:
              containers:
                - name: hello-world
                  image: "{{ metallb_test_image }}"
                  imagePullPolicy: Always
                  ports:
                    - containerPort: 8080
                      protocol: TCP

  - name: Create a service
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Service
        metadata:
          name: hello-world
          namespace: test
        spec:
          selector:
            app: hello-world
          ports:
            - port: 80
              targetPort: 8080
              protocol: TCP
          type: LoadBalancer

  - name: Check if the loadbalancer service is proper
    shell: oc get svc -n test --no-headers | awk '{if ($2 ~ /LoadBalancer/)  print $4}'
    register: metallb_svc
    until: metallb_svc.stdout != "<pending>"
    retries: 10
    delay: 40

  - name: Fetch the speaker pod name
    shell: oc get pods -n "{{ metallb_namespace }}" -o wide | awk '{if ($1 ~ /^speaker/) print $1}'
    register: speaker_pods
    failed_when: speaker_pods.stdout_lines|length != 2

  # check show-running-config
  - name: check show-running-config
    shell: oc exec -n "{{ metallb_namespace }}" {{ speaker_pods.stdout_lines[0] }} -c frr -- vtysh -c "show running-config"
    register: running_config
    failed_when: running_config.stderr != ""

  - name: check show bgp neighbor
    shell: oc exec -n "{{ metallb_namespace }}" {{ speaker_pods.stdout_lines[0] }} -c frr -- vtysh -c "show bgp neighbor"
    register: bgp_neighbor
    failed_when: bgp_neighbor.stderr != ""

  - name: Create a replica set
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: hello-world-new
          namespace: test
          labels:
            app: hello-world-new
        spec:
          selector:
            matchLabels:
              app: hello-world-new
          replicas: 2
          template:
            metadata:
              labels:
                app: hello-world-new
            spec:
              containers:
                - name: hello-world-new
                  image: "{{ metallb_test_image }}"
                  imagePullPolicy: Always
                  ports:
                    - containerPort: 8080
                      protocol: TCP

  - name: Create a service with externalTrafficPolicy 'Local'
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Service
        metadata:
          name: hello-world-new
          namespace: test
        spec:
          selector:
            app: hello-world-new
          ports:
            - port: 80
              targetPort: 8080
              protocol: TCP
          type: LoadBalancer
          externalTrafficPolicy: Local

  - name: Check if the loadbalancer service is proper
    shell: oc get svc -n default --no-headers | awk '{if ($2 ~ /hello-world-new/)  print $4}'
    register: metallb_svc
    failed_when: metallb_svc.stdout == "<pending>"

  # validate the externaltraffic policy

  - name: Create a replica set
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: hello-world-new-1
          namespace: test
          labels:
            app: hello-world-new-1
        spec:
          selector:
            matchLabels:
              app: hello-world-new-1
          replicas: 2
          template:
            metadata:
              labels:
                app: hello-world-new-1
            spec:
              containers:
                - name: hello-world-new-1
                  image: "{{ metallb_test_image }}"
                  imagePullPolicy: Always
                  ports:
                    - containerPort: 8080
                      protocol: TCP

  - name: Create a service with externalTrafficPolicy 'Local'
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Service
        metadata:
          name: hello-world-new-1
          namespace: test
        spec:
          selector:
            app: hello-world-new-1
          ports:
            - port: 80
              targetPort: 8080
              protocol: TCP
          type: LoadBalancer
          externalTrafficPolicy: Local

  - name: Check if the loadbalancer service is proper
    shell: oc get svc -n default --no-headers | awk '{if ($2 ~ /hello-world-new-1/)  print $4}'
    register: metallb_svc
    failed_when: metallb_svc.stdout == "<pending>"

  - name: Delete the replica set
    kubernetes.core.k8s:
      state: absent
      api_version: apps/v1
      kind: ReplicaSet
      name: hello-world-new-1
      namespace: test

  - name: Delete the service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: hello-world-new-1
      namespace: test

  - name: Delete the replica set
    kubernetes.core.k8s:
      state: absent
      api_version: apps/v1
      kind: ReplicaSet
      name: hello-world-new
      namespace: test

  - name: Delete the service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: hello-world-new
      namespace: test

  - name: Delete the replica set
    kubernetes.core.k8s:
      state: absent
      api_version: apps/v1
      kind: ReplicaSet
      name: hello-world
      namespace: test

  - name: Delete the service
    kubernetes.core.k8s:
      state: absent
      api_version: v1
      kind: Service
      name: hello-world
      namespace: test

  - name: Delete IP Address pool for bgp
    kubernetes.core.k8s:
      state: absent
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-silver-bgp
          namespace: "{{ metallb_namespace }}"

  - name: Delete IP Address pool for layer2
    kubernetes.core.k8s:
      state: absent
      definition:
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: address-pool-silver-l2
          namespace: "{{ metallb_namespace }}"

  - name: Delete the instance of metallb operator
    include_tasks: operator-cleanup.yml
  when: metallb_e2e
