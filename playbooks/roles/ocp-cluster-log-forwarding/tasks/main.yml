---
- name: Check URL variables
  fail:
    msg: Please check URLs for external VM instance.
  when: >
        ((syslog_url == "" or elasticsearch_url == "" or kafka_url == "" or fluentd_url == "") or
        (syslog_url == None or elasticsearch_url == None or kafka_url == None or fluentd_url == None)) and
        cluster_log_forwarder == true

- name: Check Channel variables
  fail:
    msg: Please check Channel variables for ClusterLogging and ElasticSearch Operators.
  when: >
        (cluster_logging_channel == "" or elastic_search_channel == "") or
        (cluster_logging_channel == None or elastic_search_channel == None) 

- set_fact: 
    log_labels: "{{ log_label }}"
  when: log_label != None or log_label != ""

- set_fact: 
    log_labels: "test-clf"
  when: log_label == None or log_label == ""

# Adding ImageSource and CatalogSource
- name: Create ImageContentSourcePolicy and CatalogSource
  block: 
  - name: Create CatalogSource
    template:
      src: "{{ role_path }}/templates/CatalogSource.yaml.j2"
      dest: "{{ role_path }}/files/CatalogSource.yml"

  - name: Run ImageSource
    shell: oc apply -f "{{ role_path }}/files/ImageSource.yml"
     
  - name: Run CatalogSource
    shell: oc apply -f "{{ role_path }}/files/CatalogSource.yml"
 
  - name: Check ImageSource and CatalogSource added
    shell:  oc get CatalogSource -n openshift-marketplace | grep "cluster-logging\|elasticsearch" |wc -l
    register: output

  - name: Fail if CatalogSource not added
    fail:
      msg: "CatalogSource not added !"
    when: output.stdout|int != 2
 
  - name: set fact variable for Subscription
    set_fact:
      elasticsearch_op_source: "elasticsearch"
      cluster_logging_op_source: "cluster-logging"

  when: elasticsearch_clf_cs != '' and clusterlogging_clf_cs != ''

- name: Use Default Sources
  block: 
  - name: Check ImageContentSourcePolicy exists
    shell: oc get ImageContentSourcePolicy | grep brew-registry | wc -l
    register: icsp

  - name: Check CatalogSource exists
    shell:  oc get CatalogSource -n openshift-marketplace | grep "cluster-logging\|elasticsearch" |wc -l
    register: output

  - name: Delete ImageContentSourcePolicy if exist
    shell: oc delete ImageContentSourcePolicy brew-registry
    when: icsp.stdout|int == 1

  - name: Delete CatalogSource if exists
    shell: oc delete CatalogSource elasticsearch cluster-logging -n openshift-marketplace
    when: output.stdout|int == 2

  - name: Set disableAllDefaultSources to false
    shell: |
      oc patch operatorhub.config.openshift.io/cluster -p='{"spec":{"disableAllDefaultSources":false}}' --type=merge

  - name: set fact variable for Subscription
    set_fact:
      elasticsearch_op_source: "redhat-operators"
      cluster_logging_op_source: "redhat-operators"

  when: 
    - elasticsearch_clf_cs == "" or elasticsearch_clf_cs == None
    - clusterlogging_clf_cs == "" or elasticsearch_clf_cs == None
  
  # Check if default storage class has exists
- name: Check if default storage class has exists
  shell:  oc get sc | awk '/(default)/{ print $1 }' | wc -l
  register: storage_class
  failed_when: storage_class.stdout|int == 0
  ignore_errors: yes

- name: Fail if default storage class not exist
  fail:
    msg: "Default Storage Class does not exist. Please create one"
  when: storage_class.failed

  # Create namespaces
- name: Create namespace for elastic-search and cluster logging
  openshift_raw:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ item }}"
      annotations:
        openshift.io/node-selector: ""
      labels:
        openshift.io/cluster-monitoring: "true"
  loop:
    - openshift-operators-redhat
    - openshift-logging

# Create an Operator Group object for penShift Elasticsearch Operator and Red Hat OpenShift Logging Operator
- name: Create operator gp for elastic-search
  openshift_raw:
    state: present
    definition:
      apiVersion: operators.coreos.com/v1
      kind: OperatorGroup
      metadata:
        name: openshift-operators-redhat
        namespace: openshift-operators-redhat
      spec: {}

- name: Create operator gp for cluster-logging
  openshift_raw:
    state: present
    definition:
      apiVersion: operators.coreos.com/v1
      kind: OperatorGroup
      metadata:
        name: cluster-logging
        namespace: openshift-logging
      spec:
        targetNamespaces:
        - openshift-logging

# Create a Subscription object OpenShift Elasticsearch Operator and Red Hat OpenShift Logging Operator.
- name: subscription for elastic-search operator
  openshift_raw:
    state: present
    definition:
      apiVersion: operators.coreos.com/v1alpha1
      kind: Subscription
      metadata:
        name: "elasticsearch-operator"
        namespace: "openshift-operators-redhat"
      spec:
        channel: "{{ elastic_search_channel }}"
        installPlanApproval: "Automatic"
        source: "{{ elasticsearch_op_source }}"
        sourceNamespace: "openshift-marketplace"
        name: "elasticsearch-operator"

- name: subscription for cluster-logging operator
  openshift_raw:
    state: present
    definition:
      apiVersion: operators.coreos.com/v1alpha1
      kind: Subscription
      metadata:
        name: cluster-logging
        namespace: openshift-logging
      spec:
        channel: "{{ cluster_logging_channel }}"
        name: cluster-logging
        source: "{{ cluster_logging_op_source }}"
        sourceNamespace: openshift-marketplace


# Verify the Operator installation.
- name: Verify the Operator installation
  shell:  oc get csv -n openshift-logging --no-headers | awk '{ if (($1 ~ /^cluster-logging/ || $1 ~ /^elasticsearch-operator/) && $NF=="Succeeded") print $1 }'| wc -l
  register: operators
  until: operators.stdout|int == 2
  retries: 15
  delay: 120
  ignore_errors: yes

- name: Fail the task when Operator installation failed
  fail:
    msg: "Operator not installed successfully"
  when: operators.failed


# Create a OpenShift Logging instance
- name: Create an instance for Logging Operator
  openshift_raw:
    state: present
    definition:
      apiVersion: "logging.openshift.io/v1"
      kind: "ClusterLogging"
      metadata:
        name: "instance"
        namespace: "openshift-logging"
      spec:
        managementState: "Managed"
        logStore:
          type: "elasticsearch"
          retentionPolicy:
            application:
              maxAge: 1d
            infra:
              maxAge: 7d
            audit:
              maxAge: 7d
          elasticsearch:
            nodeCount: 3
            storage:
              size: 200G
            resources:
              requests:
                memory: "8Gi"
            proxy:
              resources:
                limits:
                  memory: 256Mi
                requests:
                  memory: 256Mi
            redundancyPolicy: "SingleRedundancy"
        visualization:
          type: "kibana"
          kibana:
            replicas: 1
        curation:
          type: "curator"
          curator:
            schedule: "30 3 * * *"
        collection:
          logs:
            type: "fluentd"
            fluentd: {}

# Create a OpenShift ClutserLogForwarding dummy instance
- name: Create a dummy instance for ClutserLogForwarding
  openshift_raw:
    state: present
    definition:
      apiVersion: logging.openshift.io/v1
      kind: ClusterLogForwarder
      metadata:
        name: instance
        namespace: openshift-logging
      spec:
        outputs:
        - name: local-log-forwarder
          type: syslog
          syslog:
            appName: "local-log-forwarder"
            facility: local0
            rfc: RFC3164
            msgID: mymsg
            procID: myproc
            payloadKey: message
            severity: informational
          url: 'udp://server.east.example.com:514'
        pipelines:
        - name: audit-log-forwarder
          inputRefs:
          - audit
          outputRefs:
          - default
          parse: json
          labels:
            logtype: "{{ log_labels }}-audit"
        - name: app-log-forwarder
          inputRefs:
          - application
          outputRefs:
          - default
          parse: json
          labels:
            logtype: "{{ log_labels }}-application"
        - name: infra-log-forwarder
          inputRefs:
          - infrastructure
          outputRefs:
          - default
          labels:
            logtype: "{{ log_labels }}-infrastructure"
  register: cl_cr
  when: cluster_log_forwarder == false

# Check if deployment is successfull
- name: Check the deployment
  shell: oc get deployment -n openshift-logging | awk '{print $1}' | wc -l
  register: deployments
  until: deployments.stdout|int > 4
  retries: 10
  delay: 60

- name: Check logging Pods are restarting if CR for CL has changed
  block:
    # check pods are in restarting state
    - name: Check the logging Pods are restarting
      shell: oc get pods -n openshift-logging --no-headers | awk '{if ($3 == "Terminating" ) print $1}' | wc -l
      register: pods
      until: pods.stdout|int > 0
      retries: 10
      delay: 5
      ignore_errors: yes

    - name: Delete PODs if not restarted Automatic
      shell: oc delete pod $(kubectl get pods -n openshift-logging| grep fluent | awk '{print $1}') -n openshift-logging
      when: pods.failed and cl_cr.changed
  when: cluster_log_forwarder == false

# check pods are in good state
- name: Check the logging Pods are in good state
  shell: oc get pods -n openshift-logging --no-headers | awk '{if ($3 != "Running" && $3 != "Completed" ) print $1}' | wc -l
  register: pods
  until: pods.stdout|int == 0
  retries: 4
  delay: 60
  ignore_errors: yes
  
- name: Get Error state pods 
  shell: oc get pod -n openshift-logging | grep Error| wc -l
  register: err_pods

- name: Delete all pods in error state
  shell: oc delete pod $(kubectl get pods -n openshift-logging| grep Error | awk '{print $1}') -n openshift-logging
  when: err_pods.stdout|int != 0

# check pods are in good state
- name: Check the logging Pods are in good state
  shell: oc get pods -n openshift-logging --no-headers | awk '{if ($3 != "Running" && $3 != "Completed" ) print $1}' | wc -l
  register: pods
  until: pods.stdout|int == 0
  retries: 15
  delay: 120

# Create CLF for Elasticsearch and Syslog
- name: Create new external ClusterLogForwarder instance 
  block:
    # Create an ClusterLogForwarder instance for syslog and elasticsearch
    - name: Create an ClusterLogForwarder instance 
      include_tasks: "{{ role_path }}/files/clusterlogforwarder.yml"
      vars:
        audit_log_appname: "{{ log_labels }}-audit"
        app_log_appname: "{{ log_labels }}-application"
        infra_log_appname: "{{ log_labels }}-infrastructure"
  when: 
    - syslog_url != "" 
    - elasticsearch_url != "" 
    - kafka_url != "" 
    - fluentd_url != "" 
    - cluster_log_forwarder

# Testcase-1 Deployment of nginx pod in default namespace
- name: Create POD in default namespace
  openshift_raw:
    state: present
    definition:
      kind: Pod
      metadata:
        namespace: default
        name: nginx-pod-default
        labels:
          role: web
      spec:
        containers:
          - name: nginx-pod-default
            image: quay.io/varad_ahirwadkar/nginx-unprivileged
            ports:
            - containerPort: 80

- name: wait for nginx-pod-default pods to come up
  shell: kubectl wait --all  --namespace=default --for=condition=Ready pods nginx-pod-default --timeout=180s


#Create namespace for structured data generating app centos-logtest-
- name: Create namespace
  openshift_raw:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: test-logging

# Deployment 
- name: Deploy app centos-logtest- that generates structured data
  shell: |
    oc apply -f "{{ role_path }}/files/logtest-structured-data.yml" -n test-logging

# waiting for pods to come up
- name: wait for centos-logtest- pods to come up
  shell: kubectl wait --all  --namespace=test-logging --for=condition=Ready pods --timeout=180s


# Clone and Deploy acmeair-mainservice-java app in acme-air namespace
- name: Clone acmeair-mainservice-java repo
  git:
    repo: https://github.com/power-dev-env/acmeair-mainservice-java.git
    dest: /root/acmeair-mainservice-java/

- name: Deployment of acmeair-mainservice-java pods
  shell: |
    sh /root/acmeair-mainservice-java/scripts/deployToOpenshift.sh

- name: wait for acmeair pods to come up
  shell: |
    oc project default
    kubectl wait --all  --namespace=acme-air --for=condition=Ready pods --timeout=300s


- name: Pause for 1 minutes to get new logs 
  pause:
    minutes: 1

