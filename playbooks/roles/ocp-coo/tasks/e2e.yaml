---

- name: Invoke the role check-cluster-health to check cluster status
  include_role:
    name: check-cluster-health

- name: Create a coo logs directory
  ansible.builtin.file:
    path: "{{ coo_work_dir }}"
    state: directory
    mode: '0755'

- name: Deploy COO stack
  block:
    - name: Create a target namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ coo_e2e_namespace }}"

    - name: Create MonitoringStack
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: MonitoringStack
          metadata:
            labels:
              mso: example
            name: multi-ns
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            alertmanagerConfig:
              disabled: false
            logLevel: info
            namespaceSelector:
              matchLabels:
                monitoring.rhobs/stack: multi-ns
            prometheusConfig:
              replicas: 2
            resourceSelector:
               matchLabels:
                app: demo
                monitoring.rhobs/stack: multi-ns
            resources: {}
            retention: 120h

    - name: Create ThanosQuerier
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: ThanosQuerier
          metadata:
            name: example-thanos
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            namespaceSelector:
              matchNames:
                - "{{ coo_e2e_namespace }}"
            selector:
              matchLabels:
                mso: example

    - name: Verify Thanoquerier and MonitoringStack are created successfully
      shell: oc get pods -n "{{ coo_e2e_namespace }}" --no-headers | grep -v "Running" | wc -l
      register: mon_pods
      until: mon_pods.stdout|int == 0 and mon_pods.stderr == ""
      retries: 10
      delay: 30

  when: enable_coo_stack

- name: Deploy logging UIPlugin
  block:
    - name: Check if loki secret is present
      shell: oc get secrets -n openshift-logging | awk '{ if ($1 ~ /lokicred-secret/) print $1 }' | wc -l
      register: secret_count
      failed_when: secret_count.stdout|int == 0

    - name: Fail if secret is not present
      fail:
        msg: "Secret lokicred-secret not found in openshift-logging namespace"
      when: secret_count.stdout|int == 0

    - name: Get list of csv present in openshift-logging namespace
      shell: oc get csv -n openshift-logging -o jsonpath='{.items[*].metadata.name}'
      register: csv_list

    - name: Check if Loki Operator CSV exists
      set_fact:
        loki_installed: "{{ csv_list.stdout is contains('loki') }}"

    - name: Fail if Loki Operator is not installed 
      fail:
        msg: "Install Loki Operator"
      when: not loki_installed

    - name: Check if Cluster Logging Operator CSV exists
      set_fact:
        clo_installed: "{{ csv_list.stdout is contains('cluster-logging') }}"

    - name: Fail if Cluster Logging is not installed 
      fail:
        msg: "Install Cluster Logging Operator"
      when: not clo_installed

    - name: Get the CLO csv name
      set_fact:
        clo_csv_name: "{{ item }}"
      loop: "{{ csv_list.stdout.split() }}"
      when: item is search('cluster-logging')

    - name: Get Cluster Logging csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-logging
        name: "{{ clo_csv_name }}"
      register: clo_csv

    - name:  Set CLO version 
      set_fact:
        clo_version: "{{ clo_csv.resources[0].spec.version.split('.')[:2] | join('.') }}"

    - name: Create Lokistack instance 
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: loki.grafana.com/v1
          kind: LokiStack
          metadata:
            name: lokistack-coo
            namespace: openshift-logging
          spec:
            managementState: Managed
            size: 1x.small
            replicationFactor: 1
            storage:
              schemas:
              - effectiveDate: "2020-10-11"
                version: v11
              secret:
                name: lokicred-secret
                type: s3
            storageClassName: nfs-storage-provisioner
            tenants:
              mode: openshift-logging
            rules:
              enabled: true
              selector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
              namespaceSelector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
      when: clo_version|float < 6.0

    - name: Create Lokistack instance 
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: loki.grafana.com/v1
          kind: LokiStack
          metadata:
            name: lokistack-coo
            namespace: openshift-logging
          spec:
            managementState: Managed
            size: 1x.small
            replicationFactor: 1
            storage:
              schemas:
              - effectiveDate: "2023-10-15"
                version: v13
              secret:
                name: lokicred-secret
                type: s3
            storageClassName: nfs-storage-provisioner
            tenants:
              mode: openshift-logging
            rules:
              enabled: true
              selector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
              namespaceSelector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
      when: clo_version|float >= 6.0

    - name: Create ClusterLogging instance
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: logging.openshift.io/v1
          kind: ClusterLogging
          metadata:
            annotations:
              logging.openshift.io/preview-korrel8r-console: enabled 
            name: instance
            namespace: openshift-logging
          spec:
            managementState: Managed
            logStore:
              type: lokistack
              lokistack:
                name: lokistack-coo
            collection:
              type: "vector"
              visualization:
                ocpConsole:
                  logsLimit: 50
                  timeout:  6m
                type: ocp-console
      when: clo_version|float < 6.0

    - name: Create ClusterLogForwarder instance for CLO version greater than or equal to v6.0.0
      block:
      - name: Create ConfigMap
        kubernetes.core.k8s:
          state: present
          definition:
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: lokistack-ca
              namespace: openshift-logging
              annotations:
                service.beta.openshift.io/inject-cabundle: "true"

      - name: Create serviceAccount logcollector
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: logcollector
              namespace: openshift-logging

      - name: Create ClusterRole
        kubernetes.core.k8s:
          state: present
          definition:
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRole
            metadata:
              name: lokistack-instance-tenant-logs
            rules:
            - apiGroups:
              - 'loki.grafana.com'
              resources:
              - application
              - infrastructure
              - audit
              resourceNames:
              - logs
              verbs:
              - 'get'
              - 'create'
      - name: Create clusterrolebinding collect-application-logs
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: lokistack-instance-tenant-logs
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: lokistack-instance-tenant-logs
            subjects:
              - kind: ServiceAccount
                name: logcollector
                namespace: openshift-logging

      - name: Create ClusterLogForwarder instance
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: observability.openshift.io/v1
            kind: ClusterLogForwarder
            metadata:
              name: collector
              namespace: openshift-logging
            spec:
              managementState: Managed
              outputs:
                - lokiStack:
                    authentication:
                      token:
                        from: serviceAccount
                    target:
                      name: lokistack-coo
                      namespace: openshift-logging
                  name: lokistack
                  tls:
                    ca:
                      configMapName: lokistack-coo-gateway-ca-bundle
                      key: service-ca.crt
                  type: lokiStack
              pipelines:
                - inputRefs:
                    - infrastructure
                    - audit
                    - application
                  name: forward-to-lokistack
                  outputRefs:
                    - lokistack
              serviceAccount:
                name: logcollector
      when: clo_version|float >= 6.0

    - name: Verify Logging stack
      shell: oc get pods -n openshift-logging --no-headers | grep logging-view | grep "Running" | wc -l
      register: coo_pods
      until: coo_pods.stdout|int == 1 and coo_pods.stderr == ""
      retries: 10
      delay: 30
      when: clo_version|float < 6.0

    - name: Enable UIPlugin
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: logging
          spec:
            logging:
              logsLimit: 6
              lokiStack:
                name: lokistack-coo
              timeout: 6m
            type: Logging

    - name: Verify logging UIPlugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep logging | grep "Running\|Completed" | wc -l
      register: logging_pods
      until: logging_pods.stdout|int == 1 and logging_pods.stderr == ""
      retries: 10
      delay: 30
  when: enable_logging_uiplugin

- name: Deploy Distributed Tracing Console UI plugin
  block:
    - name: Get Tempo Operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-tempo-operator
      register: tempo_csv
    
    - name: Get OpenTelemetry Operator csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-opentelemetry-operator
      register: opentelemetry_csv

    - name: Check if Tempo Operator CSV exists
      set_fact:
        tempo_installed: "{{ tempo_csv.resources | selectattr('metadata.name', 'search', 'tempo') | list | length > 0 }}"

    - name: Fail if Tempo Operator is not installed 
      fail:
        msg: "Install Tempo Operator"
      when: not tempo_installed

    - name: Check if OpenTelemetry Operator CSV exists
      set_fact:
        opentelemetry_installed: "{{ opentelemetry_csv.resources | selectattr('metadata.name', 'search', 'opentelemetry') | list | length > 0 }}"

    - name: Fail if OpenTelemetry Operator is not installed 
      fail:
        msg: "Install OpenTelemetry Operator"
      when: not opentelemetry_installed

    - name: Create the Tracing UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: distributed-tracing
            namespace: openshift-cluster-observability-operator
          spec:
            type: DistributedTracing

    - name: Verify Tracing UI plugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep distributed-tracing- | grep "Running" | wc -l
      register: tracing_pods
      until: tracing_pods.stdout|int == 1 and tracing_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Clone Distributed Tracing QE repo
      git:
        repo: "{{ distributed_tracing_qe_repo }}"
        dest: "{{ coo_work_dir }}/distributed-tracing-console-plugin"
        version: "{{ distributed_tracing_qe_repo_branch }}"

    - name: Download and untar chainsaw
      unarchive:
        src: "{{ chainsaw_tarball }}"
        dest: "{{ chainsaw_path }}"
        remote_src: yes

    - name: Run Distributed Tracing testcases
      shell: "{{ chainsaw_path }}/chainsaw test --skip-delete {{ coo_work_dir }}/distributed-tracing-console-plugin/tests/fixtures/chainsaw-tests"
      register: tracing_tests

    - name: Save the tracing tests in a file
      copy:
        content: "{{ tracing_tests.stdout }}"
        dest: "{{ coo_work_dir }}"
  when: enable_distributed_tracing_uiplugin 

- name: Deploy troubleshootingpanel-plugin  
  block:
    - name: Get Tempo Operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-tempo-operator
      register: tempo_csv
    
    - name: Get OpenTelemetry Operator csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-opentelemetry-operator
      register: opentelemetry_csv 

    - name: Check if Tempo Operator CSV exists
      set_fact:
        tempo_installed: "{{ tempo_csv.resources | selectattr('metadata.name', 'search', 'tempo') | list | length>0 }}"

    - name: Fail if Tempo Operator is not installed 
      fail:
        msg: "Install Tempo Operator"
      when: not tempo_installed

    - name: Check if OpenTelemetry Operator CSV exists
      set_fact:
        opentelemetry_installed: "{{ opentelemetry_csv.resources | selectattr('metadata.name', 'search', 'opentelemetry') | list | length>0 }}"

    - name: Fail if OpenTelemetry Operator is not installed 
      fail:
        msg: "Install OpenTelemetry Operator"
      when: not opentelemetry_installed

    - name: Create a Deployment to create Test Data
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion:  apps/v1
          kind: Deployment
          metadata:
            name: bad-deployment
            namespace: default
          spec:
            selector:
              matchLabels:
                app: bad-deployment
            template:
              metadata:
                labels:
                  app: bad-deployment
              spec:
                containers:
                - name: bad-deployment
                  image: quay.io/openshift-logging/vector:5.8

    - name: Create TroubleShooting-Panel UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: troubleshooting-panel
          spec:
            troubleshootingPanel:
              timeout: 5m
            type: TroubleshootingPanel
 
    - name: Verify Korrel8r pod is in Running state
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep korrel8r | grep "Running" | wc -l
      register: korrel8r_pod
      until: korrel8r_pod.stdout|int == 1 and korrel8r_pod.stderr == ""
      retries: 10
      delay: 30

    - name: Verify troubleshooting-panel pod is in Running state
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep troubleshooting-panel | grep "Running" | wc -l
      register: troubleshooting_pod
      until: troubleshooting_pod.stdout|int == 1 and troubleshooting_pod.stderr == ""
      retries: 10
      delay: 30
  when: enable_troubleshootingpanel_uiplugin

- name: Deploy Incidents plugin
  block:
    - name: Enable Monitoring plugin
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: monitoring
          spec:
            type: Monitoring
            monitoring:
              incidents:
                enabled: true

    - name: Verify Monitoring UIPlugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep health-analyzer | grep "Running\|Completed" | wc -l
      register: monitoring_pods
      until: monitoring_pods.stdout|int == 1 and monitoring_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Check metrics are exposed
      shell: oc exec -n openshift-monitoring -it prometheus-k8s-0 -- curl -s "http://localhost:9090/api/v1/query?query=cluster:health:components" | less
      register: prometheus_metrics

    - name: Display prometheus metrics
      debug:
        msg: "{{ prometheus_metrics.stdout_lines }}"
  when: enable_monitoring_uiplugin

- name: Deploy Perses Dashboard
  block:
    - name: Clone Perses Dashboard
      get_url:
        url: "https://raw.githubusercontent.com/perses/perses-operator/main/config/samples/openshift/openshift-cluster-sample-dashboard.yaml"
        dest: "{{ coo_work_dir }}/perses_dashboard.yaml"  

    - name: Change the namespace of Perses dashboard
      shell: >
        sed '0,/^namespace: perses-dev$/s//namespace: openshift-cluster-observability-operator/' {{ coo_work_dir }}/perses_dashboard.yaml > {{ coo_work_dir }}/perses_dashboard.yaml.tmp
        && mv {{ coo_work_dir }}/perses_dashboard.yaml.tmp {{ coo_work_dir }}/perses_dashboard_upd.yaml
    
    - name: Create Perses Dashboard
      shell: oc apply -f {{ coo_work_dir }}/perses_dashboard_upd.yaml
    
    - name: Clone Perses datasource
      get_url:
        url: https://raw.githubusercontent.com/perses/perses-operator/main/config/samples/openshift/thanos-querier-datasource.yaml
        dest: "{{ coo_work_dir }}/perses_datasource.yaml"
    
    - name: Change the namespace of Perses Datasource
      shell: >
        sed '0,/^namespace: perses-dev$/s//namespace: openshift-cluster-observability-operator/' {{ coo_work_dir }}/perses_datasource.yaml > {{ coo_work_dir }}/perses_datasource.yaml.tmp
        && mv {{ coo_work_dir }}/perses_datasource.yaml.tmp {{ coo_work_dir }}/perses_datasource_upd.yaml

    - name: Create Perses Datasource
      shell: oc apply -f {{ coo_work_dir }}/perses_datasource_upd.yaml

    - name: Check Monitoring UIPlugin
      shell: oc get uiplugin monitoring | wc -l
      register: plugin_exits

    - name: Enable Perses
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: monitoring
          spec:
            type: Monitoring
            monitoring:
              incidents:
                enabled: true
              perses:
                enabled: true
      when: plugin_exits.stdout | int == 0

    - name: Enable Perses
      shell: >
        oc patch uiplugin monitoring --type=merge -p '{"spec":{"monitoring":{"perses":{"enabled":true}}}}'
      when: plugin_exits.stdout | int == 1
  when: enable_perses_dashboard
