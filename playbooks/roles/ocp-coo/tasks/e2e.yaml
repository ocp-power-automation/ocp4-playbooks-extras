---

- name: Invoke the role check-cluster-health to check cluster status
  include_role:
    name: check-cluster-health

- name: Create a coo logs directory
  ansible.builtin.file:
    path: "{{ coo_work_dir }}"
    state: directory
    mode: '0755'

- name: Deploy Dashboard UIPlugin
  block:
    - name: Create a target namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ coo_e2e_namespace }}"

    - name: Create MonitoringStack
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: MonitoringStack
          metadata:
            labels:
              mso: example
            name: multi-ns
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            alertmanagerConfig:
              disabled: false
            logLevel: info
            namespaceSelector:
              matchLabels:
                monitoring.rhobs/stack: multi-ns
            prometheusConfig:
              replicas: 2
            resourceSelector:
               matchLabels:
                app: demo
                monitoring.rhobs/stack: multi-ns
            resources: {}
            retention: 120h

    - name: Create ThanosQuerier
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: ThanosQuerier
          metadata:
            name: example-thanos
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            namespaceSelector:
              matchNames:
                - "{{ coo_e2e_namespace }}"
            selector:
              matchLabels:
                mso: example

    - name: Verify Thanoquerier and MonitoringStack are created successfully
      shell: oc get pods -n "{{ coo_e2e_namespace }}" --no-headers | grep -v "Running" | wc -l
      register: mon_pods
      until: mon_pods.stdout|int == 0 and mon_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Check prometheus metrics from thanoquerier
      shell: oc -n "{{ coo_e2e_namespace }}" exec deploy/thanos-querier-example-thanos \
       -- curl -k 'http://thanos-querier-example-thanos.{{ coo_e2e_namespace }}.svc:10902/api/v1/query?' --data-urlencode 'query=prometheus_build_info' | jq
      register: prometheus_op

    - name: Display prometheus metrics
      debug:
        msg: "{{ prometheus_op.stdout_lines }}"

    - name: Create UIPlugin Controller
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: dashboards 
          spec:
            type: Dashboards

    - name: Verify Dashboards plugin
      shell: oc -n openshift-cluster-observability-operator get pod | grep observability-ui-dashboards | grep "Running" | wc -l
      register: dashboard_pods
      until: dashboard_pods.stdout|int == 1 and dashboard_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Create datasource
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: prometheus-datasource-test
            namespace: openshift-config-managed
            labels:
              console.openshift.io/dashboard-datasource: 'true'
          data:
            'dashboard-datasource.yaml': |-
              kind: "Datasource"
              metadata:
                name: "prometheus-datasource-test"
                project: "openshift-config-managed"
              spec:
                plugin:
                  kind: "PrometheusDatasource"
                    spec:
                      direct_url: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"

    - name: Download dashboard config file
      get_url:
        url: "https://raw.githubusercontent.com/lihongyan1/coo_test/main/prometheus.json"
        dest: "{{ coo_work_dir }}/prometheus.json"
  
    - name: Create dashbord plugin
      shell: oc create configmap test-db-plugin-admin --from-file={{ coo_work_dir }}/prometheus.json -n openshift-config-managed

    - name: Label Config map
      shell: oc -n openshift-config-managed label cm test-db-plugin-admin console.openshift.io/dashboard=true
  when: enable_dashboard_uiplugin

- name: Deploy logging UIPlugin
  block:
    - name: Check if loki secret is present
      shell: oc get secrets -n openshift-logging | awk '{ if ($1 ~ /lokicred-secret/) print $1 }' | wc -l
      register: secret_count
      failed_when: secret_count.stdout|int == 0

    - name: Fail if secret is not present
      fail:
        msg: "Secret lokicred-secret not found in openshift-logging namespace"
      when: secret_count.stdout|int == 0

    - name: Get list of csv present in openshift-logging namespace
      shell: oc get csv -n openshift-logging -o jsonpath='{.items[*].metadata.name}'
      register: csv_list

    - name: Check if Loki Operator CSV exists
      set_fact:
        loki_installed: "{{ csv_list.stdout is contains('loki') }}"

    - name: Fail if Loki Operator is not installed 
      fail:
        msg: "Install Loki Operator"
      when: not loki_installed

    - name: Check if Cluster Logging Operator CSV exists
      set_fact:
        clo_installed: "{{ csv_list.stdout is contains('cluster-logging') }}"

    - name: Fail if Cluster Logging is not installed 
      fail:
        msg: "Install Cluster Logging Operator"
      when: not clo_installed

    - name: Get the CLO csv name
      set_fact:
        clo_csv_name: "{{ item }}"
      loop: "{{ csv_list.stdout.split() }}"
      when: item is search('cluster-logging')

    - name: Get Cluster Logging csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-logging
        name: "{{ clo_csv_name }}"
      register: clo_csv

    - name:  Set CLO version 
      set_fact:
        clo_version: "{{ clo_csv.resources[0].spec.version.split('.')[:2] | join('.') }}"

    - name: Create Lokistack instance 
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: loki.grafana.com/v1
          kind: LokiStack
          metadata:
            name: lokistack-coo
            namespace: openshift-logging
          spec:
            managementState: Managed
            size: 1x.small
            replicationFactor: 1
            storage:
              secret:
                name: lokicred-secret
                type: s3
            storageClassName: nfs-storage-provisioner
            tenants:
              mode: openshift-logging
            rules:
              enabled: true
              selector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
              namespaceSelector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'

    - name: Create ClusterLogging instance
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: "logging.openshift.io/v1"
          kind: "ClusterLogging"
          metadata:
            annotations:
              logging.openshift.io/preview-korrel8r-console: enabled 
            name: "instance"
            namespace: openshift-logging
          spec:
            managementState: "Managed"
            logStore:
              type: "lokistack"
              lokistack:
                name: lokistack-coo
            collection:
              type: "vector"
              visualization:
                ocpConsole:
                  logsLimit: 50
                  timeout:  6m
                type: ocp-console
      when: clo_version|float < 6.0

    - name: Create ClusterLogForwarder instance for CLO version greater than or equal to v6.0.0
      block:
      - name: Create serviceAccount logcollector
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: logcollector
              namespace: openshift-logging

      - name: Create clusterrolebinding collect-application-logs
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: collect-application-logs
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: collect-application-logs
            subjects:
              - kind: ServiceAccount
                name: logcollector
                namespace: openshift-logging

      - name: Create clusterrolebinding collect-infrastructure-logs
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: collect-infrastructure-logs
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: collect-infrastructure-logs
            subjects:
              - kind: ServiceAccount
                name: logcollector
                namespace: openshift-logging

      - name: Create clusterrolebinding collect-audit-logs
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: collect-audit-logs
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: collect-audit-logs
            subjects:
              - kind: ServiceAccount
                name: logcollector
                namespace: openshift-logging

      - name: Create ClusterLogForwarder instance
        kubernetes.core.k8s:
          state: present
          definition:  
            apiVersion: observability.openshift.io/v1
            kind: ClusterLogForwarder
            metadata:
              name: collector
              namespace: openshift-logging
            spec:
              managementState: Managed
              outputs:
                - lokiStack:
                    authentication:
                      token:
                        from: serviceAccount
                    target:
                      name: lokistack-coo
                      namespace: openshift-logging
                  name: lokistack
                  tls:
                    ca:
                      configMapName: lokistack-coo-gateway-ca-bundle
                      key: service-ca.crt
                  type: lokiStack
              pipelines:
                - inputRefs:
                    - infrastructure
                    - audit
                    - application
                  name: forward-to-lokistack
                  outputRefs:
                    - lokistack
              serviceAccount:
                name: logcollector
      when: clo_version|float >= 6.0

    - name: Verify Logging stack
      shell: oc get pods -n openshift-logging --no-headers | grep logging-view | grep "Running" | wc -l
      register: coo_pods
      until: coo_pods.stdout|int == 1 and coo_pods.stderr == ""
      retries: 10
      delay: 30
      when: clo_version|float < 6.0

    - name: Enable UIPlugin
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: logging
          spec:
            logging:
              logsLimit: 6
              lokiStack:
                name: lokistack-coo
              timeout: 6m
            type: Logging

    - name: Verify logging UIPlugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep logging | grep "Running\|Completed" | wc -l
      register: logging_pods
      until: logging_pods.stdout|int == 1 and logging_pods.stderr == ""
      retries: 10
      delay: 30
  when: enable_logging_uiplugin

- name: Deploy Distributed Tracing Console UI plugin
  block:
    - name: Get Tempo Operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-tempo-operator
      register: tempo_csv
    
    - name: Get OpenTelemetry Operator csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-opentelemetry-operator
      register: opentelemetry_csv 

    - name: Check if Tempo Operator CSV exists
      set_fact:
        tempo_installed: "{{ tempo_csv.resources | selectattr('metadata.name', 'search', 'tempo') | list | length > 0 }}"

    - name: Fail if Tempo Operator is not installed 
      fail:
        msg: "Install Tempo Operator"
      when: not tempo_installed

    - name: Check if OpenTelemetry Operator CSV exists
      set_fact:
        opentelemetry_installed: "{{ opentelemetry_csv.resources | selectattr('metadata.name', 'search', 'opentelemetry') | list | length > 0 }}"

    - name: Fail if OpenTelemetry Operator is not installed 
      fail:
        msg: "Install OpenTelemetry Operator"
      when: not opentelemetry_installed

    - name: Create the Tracing UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: distributed-tracing
            namespace: openshift-cluster-observability-operator
          spec:
            type: DistributedTracing

    - name: Verify Tracing UI plugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep distributed-tracing- | grep "Running" | wc -l
      register: tracing_pods
      until: tracing_pods.stdout|int == 1 and tracing_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Clone Distributed Tracing QE repo
      git:
        repo: "{{ distributed_tracing_qe_repo }}"
        dest: "{{ coo_work_dir }}/distributed-tracing-console-plugin"
        version: "{{ distributed_tracing_qe_repo_branch }}"
      delegate_to: remote
      
    - name: Install the necessary node.js modules on remote VM
      shell: npm install
      args:
        chdir: "{{ coo_work_dir }}/distributed-tracing-console-plugin/tests"
      delegate_to: remote

    - name: Download and untar chainsaw
      unarchive:
        src: "{{ chainsaw_tarball }}"
        dest: "{{ chainsaw_path }}"
    
    - name: Copy kubeconfig file to remote VM
      ansible.builtin.copy:
        src: /root/openstack-upi/auth/kubeconfig
        dest: /tmp/kubeconfig
      delegate_to: remote
    
    - name: Create config_vars file
      shell: >
        echo "CYPRESS_BASE_URL=https://$(oc get route -n openshift-console | awk '/console/ {print $2; exit}')" >> /tmp/config_vars.txt
        echo "CYPRESS_LOGIN_IDP=kube:admin" >> /tmp/config_vars.txt
        echo "CYPRESS_LOGIN_USERS=kubeadmin:$(cat /root/openstack-upi/auth/kubeadmin-password)" >> /tmp/config_vars.txt
        echo "CYPRESS_SKIP_COO_INSTALL=true" >> /tmp/config_vars.txt
    
    - name: Copy the environment variables file to remote VM
      ansible.builtin.copy:
        src: /tmp/config_vars.txt
        dest: /tmp/config_vars.txt
      delegate_to: remote
    
    - name: Load env vars from config file into Ansible variable
      shell: cat /tmp/config_vars.txt
      register: remote_env
      delegate_to: remote

    - name: Set remote env as Ansible variable
      set_fact:
        env_vars_remote: "{{ dict(remote_env.stdout_lines | map('split', '=', 1) | list) }}"
        
    - name: Replace minio endpoint
      replace:
        path: "{{ coo_work_dir }}/distributed-tracing-console-plugin/tests/fixtures/chainsaw-tests/multitenancy-rbac/00-install-storage.yaml"
        regexp: 'endpoint:\s*.*'
        replace: 'endpoint: http://minio.chainsaw-rbac.svc:9000'
      delegate_to: remote
      
    - name: Run Cypress tests and capture output without failing
      shell: npx cypress run
      args:
        chdir: "{{ coo_work_dir }}/distributed-tracing-console-plugin/tests"
      register: cypress_result
      ignore_errors: true
        
    - name: Save Cypress test output to file
      ansible.builtin.copy:
        content: "{{ cypress_result.stdout }}\n{{ cypress_result.stderr }}"
        dest: /tmp/cypress_test_output.log
      delegate_to: remote    
  when: enable_distributed_tracing_uiplugin 


- name: Deploy troubleshootingpanel-plugin  
  block:
    - name: Get Tempo Operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-tempo-operator
      register: tempo_csv
    
    - name: Get OpenTelemetry Operator csv
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-opentelemetry-operator
      register: opentelemetry_csv 

    - name: Check if Tempo Operator CSV exists
      set_fact:
        tempo_installed: "{{ tempo_csv.resources | selectattr('metadata.name', 'search', 'tempo') | list | length>0 }}"

    - name: Fail if Tempo Operator is not installed 
      fail:
        msg: "Install Tempo Operator"
      when: not tempo_installed

    - name: Check if OpenTelemetry Operator CSV exists
      set_fact:
        opentelemetry_installed: "{{ opentelemetry_csv.resources | selectattr('metadata.name', 'search', 'opentelemetry') | list | length>0 }}"

    - name: Fail if OpenTelemetry Operator is not installed 
      fail:
        msg: "Install OpenTelemetry Operator"
      when: not opentelemetry_installed

    - name: Create a Deployment to create Test Data
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion:  apps/v1
          kind: Deployment
          metadata:
            name: bad-deployment
            namespace: default
          spec:
            selector:
              matchLabels:
                app: bad-deployment
            template:
              metadata:
                labels:
                  app: bad-deployment
              spec:
                containers:
                - name: bad-deployment
                  image: quay.io/openshift-logging/vector:5.8

    - name: Create TroubleShooting-Panel UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: troubleshooting-panel
          spec:
            troubleshootingPanel:
              timeout: 5m
            type: TroubleshootingPanel
 
    - name: Verify Korrel8r pod is in Running state
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep korrel8r | grep "Running" | wc -l
      register: korrel8r_pod
      until: korrel8r_pod.stdout|int == 1 and korrel8r_pod_pod.stderr == ""
      retries: 10
      delay: 30

    - name: Verify troubleshooting-panel pod is in Running state
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep troubleshooting-panel | grep "Running" | wc -l
      register: troubleshooting_pod
      until: troubleshooting_pod.stdout|int == 1 and troubleshooting_pod.stderr == ""
      retries: 10
      delay: 30
  when: enable_troubleshootingpanel_uiplugin

- name: Deploy Incidents plugin
  block:
    - name: Enable Monitoring plugin
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: monitoring
          spec:
            type: Monitoring
            monitoring:
              incidents:
                enabled: true

    - name: Verify Monitoring UIPlugin
      shell: oc get pods -n openshift-cluster-observability-operator --no-headers | grep health-analyzer | grep "Running\|Completed" | wc -l
      register: monitoring_pods
      until: monitoring_pods.stdout|int == 1 and monitoring_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Check metrics are exposed
      shell: oc exec -n openshift-monitoring -it prometheus-k8s-0 -- curl -s "http://localhost:9090/api/v1/query?query=cluster:health:components" | less
      register: prometheus_metrics

    - name: Display prometheus metrics
      debug:
        msg: "{{ prometheus_metrics.stdout_lines }}"
  when: enable_monitoring_uiplugin

- name: Deploy Perses Dashboard
  block:
    - name: Clone Perses Dashboard
      get_url:
        url: "https://raw.githubusercontent.com/perses/perses-operator/main/config/samples/openshift/openshift-cluster-sample-dashboard.yaml"
        dest: "{{ coo_work_dir }}/perses_dashboard.yaml"  

    - name: Change the namespace of Perses dashboard
      shell: >
        sed '0,/^namespace: perses-dev$/s//namespace: openshift-cluster-observability-operator/' {{ coo_work_dir }}/perses_dashboard.yaml > {{ coo_work_dir }}/perses_dashboard.yaml.tmp
        && mv {{ coo_work_dir }}/perses_dashboard.yaml.tmp {{ coo_work_dir }}/perses_dashboard.yaml
    
    - name: Create Perses Dashboard
      shell: oc apply -f {{ coo_work_dir }}/perses_dashboard.yaml
    
    - name: Clone Perses datasource
      get_url:
        url: https://raw.githubusercontent.com/perses/perses-operator/main/config/samples/openshift/thanos-querier-datasource.yaml
        dest: "{{ coo_work_dir }}/perses_datasource.yaml"
    
    - name: Change the namespace of Perses Datasource
      shell: >
        sed '0,/^namespace: perses-dev$/s//namespace: openshift-cluster-observability-operator/' {{ coo_work_dir }}/perses_datasource.yaml > {{ coo_work_dir }}/perses_datasource.yaml.tmp
        && mv {{ coo_work_dir }}/perses_datasource.yaml.tmp {{ coo_work_dir }}/perses_datasource.yaml

    - name: Create Perses Datasource
      shell: oc apply -f {{ coo_work_dir }}/perses_datasource.yaml

    - name: Check Monitoring UIPlugin
      shell: oc get uiplugin monitoring | wc -l
      register: plugin_exits

    - name: Enable Perses
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: monitoring
          spec:
            type: Monitoring
            monitoring:
              incidents:
                enabled: true
              perses:
                enabled: true
      when: plugin_exits.stdout | int == 0

    - name: Enable Perses
      shell: >
        oc patch uiplugin monitoring --type=merge -p '{"spec":{"monitoring":{"perses":{"enabled":true}}}}'
      when: plugin_exits.stdout | int == 1
  when: enable_perses_dashboard