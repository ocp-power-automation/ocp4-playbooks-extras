---

- name: Invoke the role check-cluster-health to check cluster status
  include_role:
    name: check-cluster-health

- name: Create a coo logs directory
  ansible.builtin.file:
    path: "{{ coo_work_dir }}"
    state: directory
    mode: '0755'

- name: Deploy Dashboard UIPlugin
  block:
    - name: Create a target namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ coo_e2e_namespace }}"

    - name: Create MonitoringStack
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: MonitoringStack
          metadata:
            labels:
              mso: example
            name: multi-ns
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            alertmanagerConfig:
              disabled: false
            logLevel: info
            namespaceSelector:
              matchLabels:
                monitoring.rhobs/stack: multi-ns
            prometheusConfig:
              replicas: 2
            resourceSelector:
               matchLabels:
                app: demo
                monitoring.rhobs/stack: multi-ns
            resources: {}
            retention: 120h

    - name: Create ThanosQuerier
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.rhobs/v1alpha1
          kind: ThanosQuerier
          metadata:
            name: example-thanos
            namespace: "{{ coo_e2e_namespace }}"
          spec:
            namespaceSelector:
              matchNames:
                - "{{ coo_e2e_namespace }}"
            selector:
              matchLabels:
                mso: example

    - name: Verify Thanoquerier and MonitoringStack are created successfully
      shell: oc get pods -n "{{ coo_e2e_namespace }}" --no-headers | grep -v "Running" | wc -l
      register: mon_pods
      until: mon_pods.stdout|int == 0 and mon_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Check prometheus metrics from thanoquerier
      shell: oc -n "{{ coo_e2e_namespace }}" exec deploy/thanos-querier-example-thanos \
       -- curl -k 'http://thanos-querier-example-thanos.{{ coo_e2e_namespace }}.svc:10902/api/v1/query?' --data-urlencode 'query=prometheus_build_info' | jq
      register: prometheus_op

    - name: Display prometheus metrics
      debug:
        msg: "{{ prometheus_op.stdout_lines }}"

    - name: Create UIPlugin Controller
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: dashboards 
          spec:
            type: Dashboards

    - name: Verify Dashboards plugin
      shell: oc -n "{{ coo_namespace }}" get pod | grep observability-ui-dashboards | grep "Running" | wc -l
      register: dashboard_pods
      until: dashboard_pods.stdout|int == 1 and dashboard_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Create datasource
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: prometheus-datasource-test
            namespace: openshift-config-managed
            labels:
              console.openshift.io/dashboard-datasource: 'true'
          data:
            'dashboard-datasource.yaml': |-
              kind: "Datasource"
              metadata:
                name: "prometheus-datasource-test"
                project: "openshift-config-managed"
              spec:
                plugin:
                  kind: "PrometheusDatasource"
                    spec:
                      direct_url: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"

    - name: Download dashboard config file
      get_url:
        url: "https://raw.githubusercontent.com/lihongyan1/coo_test/main/prometheus.json"
        dest: "{{ coo_work_dir }}/prometheus.json"
  
    - name: Create dashbord plugin
      shell: oc create configmap test-db-plugin-admin --from-file={{ coo_work_dir }}/prometheus.json -n openshift-config-managed

    - name: Label Config map
      shell: oc -n openshift-config-managed label cm test-db-plugin-admin console.openshift.io/dashboard=true
  when: enable_dashboard_uiplugin

- name: Deploy logging UIPlugin
  block:
    - name: Check if loki secret is present
      shell: oc get secrets -n "{{ coo_logstack_namespace }}" | awk '{ if ($1 ~ /lokicred-secret/) print $1 }' | wc -l
      register: secret_count
      failed_when: secret_count.stdout|int == 0

    - name: Fail if secret is not present
      fail:
        msg: "Secret lokicred-secret not found in openshift-logging namespace"
      when: secret_count.stdout|int == 0

    - name: Get loki operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-operators-redhat
      register: loki_csv

    - name: Check if Loki Operator CSV exists
      set_fact:
        loki_installed: "{{ loki_csv.resources | selectattr('metadata.name', 'search', 'loki') | list | length>0 }}"

    - name: Fail if Loki Operator is not installed 
      fail:
        msg: "Install Loki Operator"
      when: not loki_installed

    - name: Get Cluster Logging csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: "{{ coo_logstack_namespace }}"
      register: clo_csv

    - name: Check if Cluster Logging Operator CSV exists
      set_fact:
        clo_installed: "{{ clo_csv.resources | selectattr('metadata.name', 'search', 'cluster-logging') | list | length>0 }}"

    - name: Fail if Cluster Logging is not installed 
      fail:
        msg: "Install Cluster Logging Operator"
      when: not clo_installed

    - name: Create Lokistack instance 
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: loki.grafana.com/v1
          kind: LokiStack
          metadata:
            name: lokistack-sample
            namespace: "{{ coo_logstack_namespace }}"
          spec:
            managementState: Managed
            size: 1x.small
            replicationFactor: 1
            storage:
              secret:
                name: lokicred-secret
                type: s3
            storageClassName: nfs-storage-provisioner
            tenants:
              mode: openshift-logging
            rules:
              enabled: true
              selector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'
              namespaceSelector:
                matchLabels:
                  openshift.io/cluster-monitoring: 'true'

    - name: Create ClusterLogging instance
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: "logging.openshift.io/v1"
          kind: "ClusterLogging"
          metadata:
            annotations:
              logging.openshift.io/preview-korrel8r-console: enabled 
            name: "instance"
            namespace: "{{ coo_logstack_namespace }}"
          spec:
            managementState: "Managed"
            logStore:
              type: "lokistack"
              lokistack:
                name: lokistack-sample
            collection:
              type: "vector"
              visualization:
                ocpConsole:
                  logsLimit: 50
                  timeout:  6m
                type: ocp-console

    - name: Verify Logging stack
      shell: oc get pods -n "{{ coo_logstack_namespace }}" --no-headers | grep logging-view | grep "Running" | wc -l
      register: coo_pods
      until: coo_pods.stdout|int == 1 and coo_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Enable UIPlugin
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: logging
          spec:
            logging:
              logsLimit: 20
              lokiStack:
                name: lokistack-sample
              timeout: 6m
            type: Logging

    - name: Verify logging UIPlugin
      shell: oc get pods -n "{{ coo_namespace }}" --no-headers | grep logging | grep -v "Running\|Completed" | wc -l
      register: logging_pods
      until: logging_pods.stdout|int == 0 and logging_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Verify there are no logging-view-plugin pods under "{{ coo_namespace }}"
      shell: oc get pods -n "{{ coo_namespace }}" --no-headers | grep logging-view-plugin | wc -l
      register: logging_view_pods
      until: logging_view_pods.stdout|int == 0
      retries: 10
      delay: 10
  when: enable_logging_uiplugin

- name: Deploy Distributed Tracing Console UI plugin
  block:
    - name: Get Tempo Operator csv 
      k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-tempo-operator
      register: tempo_csv

    - name: Check if Tempo Operator CSV exists
      set_fact:
        tempo_installed: "{{ tempo_csv.resources | selectattr('metadata.name', 'search', 'tempo') | list | length>0 }}"

    - name: Fail if Tempo Operator is not installed 
      fail:
        msg: "Install Tempo Operator"
      when: not tempo_installed

    - name: Create the Tracing UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: distributed-tracing
            namespace: "{{ coo_namespace }}"
          spec:
            type: DistributedTracing

    - name: Verify Tracing UI plugin
      shell: oc get pods -n {{ coo_namespace }} --no-headers | grep distributed-tracing- | grep "Running" | wc -l
      register: tracing_pods
      until: tracing_pods.stdout|int == 1 and tracing_pods.stderr == ""
      retries: 10
      delay: 30

    - name: Clone Distributed Tracing QE repo
      git:
        repo: "{{ distributed_tracing_qe_repo }}"
        dest: "{{ coo_work_dir }}/distributed-tracing-qe"
        version: "{{ distributed_tracing_qe_repo_branch }}"

    - name: Download and untar chainsaw
      unarchive:
        src: "{{ chainsaw_tarball }}"
        dest: "{{ chainsaw_path }}"
        remote_src: yes
    
    - name: Replace minio deployment image with quay_minio
      replace:
        path: "{{ coo_work_dir }}/distributed-tracing-qe/tests/e2e-acceptance/multitenancy/00-install-storage.yaml"
        regexp: 'image:\s*.*'
        replace: 'image: {{ minio_image }}'

    - name: Run Distributed Tracing testcases
      shell: "{{ chainsaw_path }}/chainsaw test --apply-timeout 2m --assert-timeout 5m --skip-delete {{ coo_work_dir }}/distributed-tracing-qe/tests/e2e-acceptance/multitenancy \
            {{ coo_work_dir }}/distributed-tracing-qe/tests/e2e-acceptance/monolithic-multitenancy-openshift/"
            
  environment: "{{ coo_e2e_env }}"
  when: enable_distributed_tracing_uiplugin 

- name: Deploy troubleshootingpanel-plugin  
  block:
    - name: Create a Deployment to create Test Data
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion:  apps/v1
          kind: Deployment
          metadata:
            name: bad-deployment
            namespace: default
          spec:
            selector:
              matchLabels:
                app: bad-deployment
            template:
              metadata:
                labels:
                  app: bad-deployment
              spec:
                containers:
                - name: bad-deployment
                  image: quay.io/openshift-logging/vector:5.8

    - name: Create TroubleShooting-Panel UIPlugin CR.
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: observability.openshift.io/v1alpha1
          kind: UIPlugin
          metadata:
            name: troubleshooting-panel
          spec:
            troubleshootingPanel:
              timeout: 5m
            type: TroubleshootingPanel
 
    - name: Verify Korrel8r pod is in Running state
      shell: oc get pods -n {{ coo_namespace }} --no-headers | grep korrel8r | grep "Running" | wc -l
      register: korrel8r_pod
      until: korrel8r_pod.stdout|int == 1 and korrel8r_pod_pod.stderr == ""
      retries: 10
      delay: 30

    - name: Verify troubleshooting-panel pod is in Running state
      shell: oc get pods -n {{ coo_namespace }} --no-headers | grep troubleshooting-panel | grep "Running" | wc -l
      register: troubleshooting_pod
      until: troubleshooting_pod.stdout|int == 1 and troubleshooting_pod.stderr == ""
      retries: 10
      delay: 30
  when: enable_troubleshootingpanel_uiplugin
